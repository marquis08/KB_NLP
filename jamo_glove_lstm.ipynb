{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import operator \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "import torch\n",
    "from torch import nn, cuda\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, Subset, DataLoader\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler, LambdaLR, ReduceLROnPlateau\n",
    "\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = cuda.is_available()\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Glove_200_PATH = '../KB_NLP/glove_txt/glove.200D.10E.txt'\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceBucketCollator():\n",
    "    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n",
    "        self.choose_length = choose_length\n",
    "        self.sequence_index = sequence_index\n",
    "        self.length_index = length_index\n",
    "        self.label_index = label_index\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        batch = [torch.stack(x) for x in list(zip(*batch))]\n",
    "        \n",
    "        sequences = batch[self.sequence_index]\n",
    "        lengths = batch[self.length_index]\n",
    "        \n",
    "        length = self.choose_length(lengths)\n",
    "        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n",
    "        padded_sequences = sequences[:, mask]\n",
    "        \n",
    "        batch[self.sequence_index] = padded_sequences\n",
    "        \n",
    "        if self.label_index is not None:\n",
    "            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n",
    "    \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \n",
    "#     stopwords = ['XXX', '.', '을', '를', '이', '가', '-', '(', ')', ':', '!', '?', ')-', '.-', 'ㅡ', 'XXXXXX', '..', '.(', '은', '는']\n",
    "#     text = re.sub(\".\", \" \", text)\n",
    "    text = re.sub(\"XXX\", \" \", text)\n",
    "    text = re.sub(\"XXXXXX\", \" \", text)\n",
    "    text = re.sub(\"[^ .?!/@$%~|0-9|ㄱ-ㅣ가-힣]+\", \"\", text) # 한글과 띄어쓰기, 특수기호 일부를 제외한 모든 글자\n",
    "#     text = re.sub(\"[\\s]+\", \"\", text.strip()) # white space duplicate\n",
    "#     text = re.sub(\"[\\.]+\", \"\", text.strip()) # full stop duplicate\n",
    "    \n",
    "    text = re.sub(\"(?s)<ref>.+?</ref>\", \"\", text) # remove reference links\n",
    "    text = re.sub(\"(?s)<[^>]+>\", \"\", text) # remove html tags\n",
    "    text = re.sub(\"&[a-z]+;\", \"\", text) # remove html entities\n",
    "    text = re.sub(\"(?s){{.+?}}\", \"\", text) # remove markup tags\n",
    "    text = re.sub(\"(?s){.+?}\", \"\", text) # remove markup tags\n",
    "    text = re.sub(\"(?s)\\[\\[([^]]+\\|)\", \"\", text) # remove link target strings\n",
    "    text = re.sub(\"(?s)\\[\\[([^]]+\\:.+?]])\", \"\", text) # remove media links\n",
    "    \n",
    "    text = re.sub(\"[']{5}\", \"\", text) # remove italic+bold symbols\n",
    "    text = re.sub(\"[']{3}\", \"\", text) # remove bold symbols\n",
    "    text = re.sub(\"[']{2}\", \"\", text) # remove italic symbols\n",
    "    text = re.sub(r'\\d+', ' ', text) # clean numbers\n",
    "    \n",
    "#     text = re.sub(r\"[^ \\r\\n\\p{Hangul}.?!]\", \" \", text) # Replace unacceptable characters with a space.\n",
    "#     text = re.sub(\"[ ]{2,}\", \" \", text) # Squeeze spaces.\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        lstm_hidden_size = 120\n",
    "        gru_hidden_size = 60\n",
    "        self.gru_hidden_size = gru_hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(*embedding_matrix.shape)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = nn.Dropout2d(0.2)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_matrix.shape[1], lstm_hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru = nn.GRU(lstm_hidden_size * 2, gru_hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.linear = nn.Linear(gru_hidden_size * 6, 20)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(20, 1)\n",
    "        \n",
    "    def apply_spatial_dropout(self, h_embedding):\n",
    "        h_embedding = h_embedding.transpose(1, 2).unsqueeze(2)\n",
    "        h_embedding = self.embedding_dropout(h_embedding).squeeze(2).transpose(1, 2)\n",
    "        return h_embedding\n",
    "\n",
    "    def forward(self, x, normal_feats, lengths=None):\n",
    "        h_embedding = self.embedding(x.long())\n",
    "        h_embedding = self.apply_spatial_dropout(h_embedding)\n",
    "\n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        h_gru, hh_gru = self.gru(h_lstm)\n",
    "\n",
    "        hh_gru = hh_gru.view(-1, self.gru_hidden_size * 2)\n",
    "\n",
    "        avg_pool = torch.mean(h_gru, 1)\n",
    "        max_pool, _ = torch.max(h_gru, 1)\n",
    "\n",
    "#         normal_linear  = F.relu(self.normal_linear(normal_feats.float()))\n",
    "\n",
    "        conc = torch.cat((hh_gru, avg_pool, max_pool), 1)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.dropout(conc)\n",
    "        out = self.out(conc)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_epochs=4, accumulation_step=2, **kwargs):\n",
    "    \n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n",
    "    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n",
    "    \n",
    "    best_epoch = -1\n",
    "    best_valid_score = 0.\n",
    "    best_valid_loss = 1.\n",
    "    all_train_loss = []\n",
    "    all_valid_loss = []\n",
    "    total_preds = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = train_one_epoch(model, criterion, train_loader, optimizer, accumulation_step)\n",
    "        val_loss, val_score = validation(model, criterion, valid_loader)\n",
    "    \n",
    "#         if val_score > best_valid_score:\n",
    "#             best_valid_score = val_score\n",
    "#             torch.save(model.state_dict(), 'best_score{}.pt'.format(fold))\n",
    "    \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        lr = [_['lr'] for _ in optimizer.param_groups]\n",
    "        print(\"Epoch {} - train_loss: {:.6f}  val_loss: {:.6f}  val_score: {:.6f}  lr: {:.5f}  time: {:.0f}s\".format(\n",
    "                epoch+1, train_loss, val_loss, val_score, lr[0], elapsed))\n",
    "\n",
    "        # inference\n",
    "        test_preds = inference_test(model, test_loader)\n",
    "        total_preds.append(test_preds)\n",
    "        \n",
    "        # scheduler update\n",
    "        scheduler.step()\n",
    "    \n",
    "    total_preds = np.average(total_preds, weights=checkpoint_weights, axis=0)\n",
    "\n",
    "    return total_preds, val_score, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_test(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    test_preds = np.zeros((len(test_dataset), 1))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, inputs in enumerate(test_loader):\n",
    "            if use_cuda:\n",
    "                inputs[0] = inputs[0].cuda()\n",
    "                inputs[1] = inputs[1].cuda()\n",
    "#                 inputs[2] = inputs[2].cuda()\n",
    "\n",
    "            outputs = model(inputs[0], inputs[1])\n",
    "#             outputs = model(inputs[0], inputs[1], inputs[2])\n",
    "            test_preds[i * batch_size:(i+1) * batch_size] = sigmoid(outputs.cpu().numpy())\n",
    "    \n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, criterion, train_loader, optimizer, accumulation_step=2):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "#     for i, (inputs, targets) in tqdm(enumerate(train_loader), desc='train', total=len(train_loader)):\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs[0] = inputs[0].cuda()\n",
    "            inputs[1] = inputs[1].cuda()\n",
    "#             inputs[2] = inputs[2].cuda()\n",
    "            targets = targets.cuda()    \n",
    "            \n",
    "        preds = model(inputs[0], inputs[1])\n",
    "        preds = model(inputs[0], inputs[1])\n",
    "        loss = criterion(preds, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        if accumulation_step:\n",
    "            if (i+1) % accumulation_step == 0:  \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item() / len(train_loader)\n",
    "        \n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def validation(model, criterion, valid_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    valid_preds = np.zeros((len(valid_dataset), 1))\n",
    "    valid_targets = np.zeros((len(valid_dataset), 1))\n",
    "    val_loss = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "#         for i, (inputs, targets) in tqdm(enumerate(valid_loader), desc='valid', total=len(valid_loader)):\n",
    "        for i, (inputs, targets) in enumerate(valid_loader):\n",
    "            \n",
    "            valid_targets[i * batch_size: (i+1) * batch_size] = targets.numpy().copy()\n",
    "            \n",
    "            if use_cuda:\n",
    "                inputs[0] = inputs[0].cuda()\n",
    "                inputs[1] = inputs[1].cuda()\n",
    "#                 inputs[2] = inputs[2].cuda()\n",
    "                targets = targets.cuda()   \n",
    "            \n",
    "            outputs = model(inputs[0], inputs[1])\n",
    "#             outputs = model(inputs[0], inputs[1], inputs[2])\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            valid_preds[i * batch_size: (i+1) * batch_size] = sigmoid(outputs.detach().cpu().numpy())\n",
    "            \n",
    "            val_loss += loss.item() / len(valid_loader)\n",
    "    \n",
    "    val_score = roc_auc_score(valid_targets, valid_preds)\n",
    "#     valid_preds = np.where(valid_preds>=0.1, 1, 0)\n",
    "#     val_score = f1_score(valid_targets, valid_preds)\n",
    "    \n",
    "    \n",
    "    return val_loss, val_score   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose=True):\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def check_coverage(vocab, embedding_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embedding_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "    \n",
    "    print(\"Found embeddings for {:.2%} of vocab\".format(len(a) / len(vocab)))\n",
    "    print(\"Found embedding for {:.2%} of all text\".format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x\n",
    "\n",
    "\n",
    "def build_matrix(word_index, word2vec_vocab, vector_size=200):\n",
    "    embedding_matrix = np.zeros((max_features + 1, vector_size))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i <= max_features:\n",
    "            try:\n",
    "                embedding_matrix[i] = word2vec_vocab[word]\n",
    "            except:\n",
    "                unknown_words.append(word)\n",
    "                \n",
    "    return embedding_matrix, unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_regexp_occ(regexp=\"\", text=None):\n",
    "    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n",
    "    # reference: https://www.kaggle.com/coolcoder22/lightgbm-fast-compact-solution\n",
    "    return len(re.findall(regexp, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_feature_engineering(text_list, month_infos):\n",
    "    add_feats_matrix = np.zeros((len(text_list), 4))\n",
    "\n",
    "    for idx in range(len(text_list)):\n",
    "        target_text = text_list[idx]    \n",
    "        add_feats_matrix[idx, 0] = len(target_text.split(' '))\n",
    "        add_feats_matrix[idx, 1] = len(set(target_text.split(' ')))\n",
    "        add_feats_matrix[idx, 2] = add_feats_matrix[idx, 1] / add_feats_matrix[idx, 0] \n",
    "\n",
    "    for idx, month in enumerate(month_infos):\n",
    "        add_feats_matrix[idx, 3]  = month\n",
    "\n",
    "    add_feats_matrix = add_feats_matrix.astype('int')\n",
    "    return add_feats_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index)+1,200))\n",
    "    unknown_words = []\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train, test, loss_fn, output_dim, lr=0.001,\n",
    "                batch_size=16, n_epochs=4, enable_checkpoint_ensemble=True):\n",
    "    param_lrs = [{'params':param,'lr':lr} for param in model.parameters()]\n",
    "    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6**epoch)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "    all_test_preds = []\n",
    "    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n",
    "\n",
    "    best_epoch = -1\n",
    "    best_valid_score = 0\n",
    "    best_valid_loss = 1\n",
    "    all_train_loss = []\n",
    "    all_valid_loss = []\n",
    "    total_preds = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        t1 = time.time()\n",
    "\n",
    "        ###### Train ##############################################\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs[0] = inputs[0].cuda()\n",
    "            inputs[1] = inputs[1].cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "            preds = model(inputs[0], inputs[1])\n",
    "            preds = model(inputs[0], inputs[1])\n",
    "            loss = criterion(preds, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            if accumulation_step:\n",
    "                if (i+1) % accumulation_step ==0:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item()/len(train_loader)\n",
    "\n",
    "        ###### Validation ########################################\n",
    "        model.eval()\n",
    "        valid_preds = np.zeros((len(valid_dataset), 1))\n",
    "        valid_targets = np.zeros((len(valid_dataset), 1))\n",
    "        val_loss = 0.\n",
    "\n",
    "        with torch.no_grad():\n",
    "    #         for i, (inputs, targets) in tqdm(enumerate(valid_loader), desc='valid', total=len(valid_loader)):\n",
    "            for i, (inputs, targets) in enumerate(valid_loader):\n",
    "\n",
    "                valid_targets[i * batch_size: (i+1) * batch_size] = targets.numpy().copy()\n",
    "\n",
    "                if use_cuda:\n",
    "                    inputs[0] = inputs[0].cuda()\n",
    "                    inputs[1] = inputs[1].cuda()\n",
    "    #                 inputs[2] = inputs[2].cuda()\n",
    "                    targets = targets.cuda()   \n",
    "\n",
    "                outputs = model(inputs[0], inputs[1])\n",
    "    #             outputs = model(inputs[0], inputs[1], inputs[2])\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                valid_preds[i * batch_size: (i+1) * batch_size] = sigmoid(outputs.detach().cpu().numpy())\n",
    "\n",
    "                val_loss += loss.item() / len(valid_loader)\n",
    "\n",
    "        val_score = roc_auc_score(valid_targets, valid_preds)\n",
    "    #     valid_preds = np.where(valid_preds>=0.1, 1, 0)\n",
    "    #     val_score = f1_score(valid_targets, valid_preds)\n",
    "        elapsed = time.time()-t1\n",
    "        lr = [_['lr'] for _ in optimizer.param_groups]\n",
    "        print(\"Epoch {} - train_loss: {:.6f} - val_loss: {:.6f} - val_score: {:.6f} lr: {:.5f} time: {:.0f}s\").format(\n",
    "        epoch+1, train_loss, val_loss, val_score, lr[0], elapsed)\n",
    "        ####### Prediction ####################################\n",
    "        model.eval()\n",
    "        test_preds = np.zeros((len(test_dataset),1))\n",
    "        with torch.no_grad():\n",
    "            for i, inputs in enumerate(test_loader):\n",
    "                inputs[0] = inputs[0].cuda()\n",
    "                inputs[1] = inputs[1].cuda()\n",
    "                outputs = model(inputs[0], inputs[1])\n",
    "                test_preds[i*batch_size:(i+1)*batch_size] = sigmoid(outputs.cpu().numpy())\n",
    "        \n",
    "        total_preds.append(test_preds)\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    total_preds = np.average(total_preds, weights=checkpoint_weights, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jamo_train.csv', 'jamo_test.csv']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../KB_NLP/jamo_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../KB_NLP/jamo_data/jamo_train.csv')\n",
    "test_df = pd.read_csv('../KB_NLP/jamo_data/jamo_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_list = train_df['jamo'].values\n",
    "test_text_list = test_df['jamo'].values\n",
    "\n",
    "train_month_infos = train_df['year_month'].apply(lambda x: int(x[-2:])).values\n",
    "test_month_infos = test_df['year_month'].apply(lambda x: int(x[-2:])).values\n",
    "\n",
    "train_add_feats_matrix = normal_feature_engineering(train_text_list, train_month_infos)\n",
    "test_add_feats_matrix = normal_feature_engineering(test_text_list, test_month_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df['jamo']\n",
    "y_train = train_df['smishing']\n",
    "x_test = test_df['jamo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(list(train_df['jamo']) + list(test_df['jamo']))\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330467"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = None\n",
    "max_features = max_features or len(tokenizer.word_index) + 1\n",
    "max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a17547d01547458c7e01215676e736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n unknown words :  291576\n"
     ]
    }
   ],
   "source": [
    "glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, Glove_200_PATH)\n",
    "print('n unknown words : ',len(unknown_words_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330467, 200)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_torch = torch.tensor(x_train, dtype=torch.long).cuda()\n",
    "x_test_torch = torch.tensor(x_test, dtype=torch.long).cuda()\n",
    "y_train_torch = torch.tensor(y_train).float().unsqueeze(1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-cb94ddbb137e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m test_preds = train_model(model, train_dataset, test_dataset, output_dim=y_train_torch.shape[-1], \n\u001b[0;32m---> 12\u001b[0;31m                          loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mall_test_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-0722b3ba7de6>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train, test, loss_fn, output_dim, lr, batch_size, n_epochs, enable_checkpoint_ensemble)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-0bf75de49e6d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, normal_feats, lengths)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormal_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mh_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mh_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_spatial_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mh_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-0bf75de49e6d>\u001b[0m in \u001b[0;36mapply_spatial_dropout\u001b[0;34m(self, h_embedding)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_spatial_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mh_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mh_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mh_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "train_dataset = TensorDataset(x_train_torch, y_train_torch)\n",
    "test_dataset = TensorDataset(x_test_torch)\n",
    "\n",
    "all_test_preds = []\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "model = NeuralNet(glove_matrix)\n",
    "model.cuda()\n",
    "\n",
    "test_preds = train_model(model, train_dataset, test_dataset, output_dim=y_train_torch.shape[-1], \n",
    "                         loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n",
    "all_test_preds.append(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df['jamo']\n",
    "y_train = train_df['smishing']\n",
    "x_test = test_df['jamo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(list(x_train)+list(x_test))\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_list = train_df['jamo'].values\n",
    "test_text_list = test_df['jamo'].values\n",
    "\n",
    "train_month_infos = train_df['year_month'].apply(lambda x: int(x[-2:])).values\n",
    "test_month_infos = test_df['year_month'].apply(lambda x: int(x[-2:])).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_add_feats_matrix = normal_feature_engineering(train_text_list, train_month_infos)\n",
    "test_add_feats_matrix = normal_feature_engineering(test_text_list, test_month_infos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
